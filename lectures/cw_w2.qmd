
## Bramki i funkcje

Napisać kod obiektowy, który pobiera string (treść do zaszyfrowania) i przekłada go na zapis bitowy. 
Dodatkowo wylosuj klucz do zaszyfrowania wiadomości (losowy ciąg 0 i 1).

Metoda 1 - szyfrowanie - Zaszyfruj wiadomość wykorzystując operacje `XOR`. 

Metoda 2 - wyświetl zaszyfrowaną wiadomość w postaci normalnej. 

Metoda 3 - odszyfruj wiadomość (ciąg zaszyfrowanego kodu binarnego)

Metoda 4 - wyświetl odszyfrowaną wiadomość


$$
\newcommand{\bra}[1]{\left \langle #1 \right \rvert}
\newcommand{\ket}[1]{\left \rvert #1 \right \rangle}
\newcommand{\braket}[2]{\left \langle #1 \middle \rvert #2 \right \rangle}
$$

### PyTorch Regresja liniowa

```{python}
import torch
import numpy as np
import matplotlib.pyplot as plt

class LinearRegression(torch.nn.Module):

    def __init__(self, inputSize, outputSize):
        super(LinearRegression, self).__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(inputSize, outputSize)
        ) 
        
    def forward(self, x):
        return self.layers(x)

# zbior danych
x = range(11)
y = [2*xi - 1 for xi in x]

# dostosowanie do pytorch
x = np.array(x, dtype=np.float32)
y = np.array(y, dtype=np.float32)

X_train = torch.from_numpy(x).view(-1,1)
y_train = torch.from_numpy(y).view(-1,1)

# obiekt liniowej regresji w wersji sieci nn
lr_model = LinearRegression(1,1)

criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)


print(lr_model)
num_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)
print(f"liczba trenowalnych parametrów: {num_params}")

epochs = 400
# petla uczaca 
for epoch in range(epochs):
    lr_model.train()

    y_pred = lr_model(X_train)
    loss = criterion(y_pred, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 50 == 0:
        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.2f}')
 
    lr_model.eval()

# po treningu jeszcze raz generujemy predykcje
lr_model.eval()
with torch.no_grad():
    predicted = lr_model(X_train)

print(lr_model.layers[0].weight, lr_model.layers[0].bias)

plt.clf()
plt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)
plt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)
plt.legend(loc='best')
plt.show()
```

### PyTorch Regresja logistyczna

```{python}
import torch
import numpy as np
from sklearn.datasets import make_classification

class LogisticRegression(torch.nn.Module):

    def __init__(self, inputSize, outputSize):
        super(LogisticRegression, self).__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(inputSize, outputSize),
            torch.nn.Sigmoid()
        )

    def forward(self, x):
        logits = self.layers(x)
        return logits


# prepare dataset
X, y = make_classification(n_samples=10**4, n_features=10 ,random_state=42)

X_train = torch.from_numpy(X.astype(np.float32))
y_train = torch.from_numpy(y.astype(np.float32))

y_train = y_train.view(y_train.shape[0], 1)

model = LogisticRegression(X_train.shape[1], y_train.shape[1])

learningRate = 0.01
criterion = torch.nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)

# petla uczaca 
num_epochs = 800

for epoch in range(num_epochs):
    # forward pass and loss
    model.train()
    y_predicted = model(X_train)
    loss = criterion(y_predicted, y_train)
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    model.eval()

    if (epoch+1) % 50 == 0:
        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')



model.eval()
with torch.no_grad():
    y_predicted = model(X_train)  # no need to call model.forward()
    y_predicted_cls = y_predicted.round()   # round off to nearest class
    acc = y_predicted_cls.eq(y_train).sum() / float(y_train.shape[0])  # accuracy
    print(f'accuracy = {acc:.4f}')
    print(f"predykcja dla wiersza 0:{y_predicted[0]}, wartosc prawdziwa: {y_train[0]}")

```